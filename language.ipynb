{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dmsbq-1OiurAEhN_-Yk63kl3oGvLQYSQ",
      "authorship_tag": "ABX9TyMm0Lezwbje8sLbWvTg+hyo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/langmodel/blob/main/language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://www.example.com'\n",
        "response = requests.get(url)\n",
        "text_data = response.text\n",
        "\n",
        "# 텍스트 데이터를 파일로 저장\n",
        "with open('text_data.txt', 'w') as f:\n",
        "    f.write(text_data)\n"
      ],
      "metadata": {
        "id": "EJMmelM7vs8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWcIrTv0Fegc",
        "outputId": "e58efb8f-d9c1-47a7-def5-a2331a521b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('wikisent2.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# 소문자 변환\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# 구두점 및 불필요한 공백 제거\n",
        "text_data = re.sub('[%s]' % re.escape(string.punctuation), '', text_data)\n",
        "text_data = re.sub('\\s+', ' ', text_data)\n",
        "\n",
        "# 단어 토큰화\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(text_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('wikisent2.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "# 입력 시퀀스 생성\n",
        "seq_length = 50\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences], maxlen=seq_length+1, padding='pre')\n",
        "\n",
        "# 입력 시퀀스와 타깃 시퀀스 생성\n",
        "input_sequences = sequences[:, :-1]\n",
        "target_sequences = sequences[:, 1:]\n",
        "\n",
        "# 클래스의 개수 정의\n",
        "num_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 언어 모델 아키텍처 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "input_sequences = input_sequences.reshape(-1, 50, 1)\n",
        "model.fit(input_sequences, tf.keras.utils.to_categorical(target_sequences, num_classes=num_classes), batch_size=128, epochs=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUSKwaaz9Fw",
        "outputId": "89ddb9c8-6ded-4d9e-b23d-c8b4883342cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 10s 10s/step - loss: 12.5332 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 1s 897ms/step - loss: 12.5188 - accuracy: 0.0600\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 12.4937 - accuracy: 0.0600\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 12.4527 - accuracy: 0.0600\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 12.3942 - accuracy: 0.0600\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 12.3177 - accuracy: 0.0600\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 12.2246 - accuracy: 0.0200\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 1s 992ms/step - loss: 12.1178 - accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 11.9982 - accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 11.8664 - accuracy: 0.0000e+00\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.7220 - accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 11.5649 - accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 11.3951 - accuracy: 0.0200\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 11.2130 - accuracy: 0.0200\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.0196 - accuracy: 0.0200\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 10.8167 - accuracy: 0.0200\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 10.6061 - accuracy: 0.0200\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 10.3899 - accuracy: 0.0200\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 10.1700 - accuracy: 0.0200\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 1s 961ms/step - loss: 9.9480 - accuracy: 0.0200\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 9.7249 - accuracy: 0.0200\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.5018 - accuracy: 0.0400\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 9.2790 - accuracy: 0.0400\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.0570 - accuracy: 0.0600\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.8359 - accuracy: 0.0600\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 8.6156 - accuracy: 0.0600\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 8.3962 - accuracy: 0.0600\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.1779 - accuracy: 0.0600\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 7.9607 - accuracy: 0.0600\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.7448 - accuracy: 0.0600\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.5302 - accuracy: 0.0600\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 7.3169 - accuracy: 0.0600\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 7.1051 - accuracy: 0.0600\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.8951 - accuracy: 0.0600\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 6.6873 - accuracy: 0.0600\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.4827 - accuracy: 0.0600\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.2818 - accuracy: 0.0600\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 6.0852 - accuracy: 0.0600\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.8933 - accuracy: 0.0600\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.7071 - accuracy: 0.0600\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 5.5277 - accuracy: 0.0600\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 1s 969ms/step - loss: 5.3563 - accuracy: 0.0600\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.1937 - accuracy: 0.0600\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 1s 943ms/step - loss: 5.0406 - accuracy: 0.0600\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.8977 - accuracy: 0.0600\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 1s 914ms/step - loss: 4.7656 - accuracy: 0.0600\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.6445 - accuracy: 0.0600\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.5344 - accuracy: 0.0600\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.4352 - accuracy: 0.0600\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.3463 - accuracy: 0.0600\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 4.2672 - accuracy: 0.0600\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 4.1971 - accuracy: 0.0600\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 4.1353 - accuracy: 0.0600\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 4.0808 - accuracy: 0.0600\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.0328 - accuracy: 0.0600\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 3.9907 - accuracy: 0.0600\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 3.9535 - accuracy: 0.0600\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 3.9208 - accuracy: 0.0600\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8918 - accuracy: 0.0600\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8661 - accuracy: 0.0600\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8432 - accuracy: 0.0600\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.8230 - accuracy: 0.0600\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.8050 - accuracy: 0.0600\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7890 - accuracy: 0.0600\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7747 - accuracy: 0.0600\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7620 - accuracy: 0.0600\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7507 - accuracy: 0.0600\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 1s 970ms/step - loss: 3.7407 - accuracy: 0.0600\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7318 - accuracy: 0.0600\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7239 - accuracy: 0.0600\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 1s 973ms/step - loss: 3.7169 - accuracy: 0.0600\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7108 - accuracy: 0.0600\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7054 - accuracy: 0.0600\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7006 - accuracy: 0.0600\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6963 - accuracy: 0.0600\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 3.6925 - accuracy: 0.0600\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6890 - accuracy: 0.0600\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 3.6858 - accuracy: 0.0600\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6830 - accuracy: 0.0600\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 3.6804 - accuracy: 0.0600\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6780 - accuracy: 0.0600\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 3.6758 - accuracy: 0.0600\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6737 - accuracy: 0.0600\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6716 - accuracy: 0.0600\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 1s 904ms/step - loss: 3.6697 - accuracy: 0.0600\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6679 - accuracy: 0.0600\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6661 - accuracy: 0.0600\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6644 - accuracy: 0.0600\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 3.6628 - accuracy: 0.0600\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 1s 970ms/step - loss: 3.6613 - accuracy: 0.0600\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6598 - accuracy: 0.0600\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6584 - accuracy: 0.0600\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6571 - accuracy: 0.0600\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 3.6558 - accuracy: 0.0600\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 1s 985ms/step - loss: 3.6547 - accuracy: 0.0600\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6535 - accuracy: 0.0600\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 3.6525 - accuracy: 0.0600\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6515 - accuracy: 0.0600\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6505 - accuracy: 0.0600\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6497 - accuracy: 0.0600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbbe383490>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('wikisent2.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "# 입력 시퀀스 생성\n",
        "seq_length = 50\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences], maxlen=seq_length+1, padding='pre')\n",
        "\n",
        "# 입력 시퀀스와 타깃 시퀀스 생성\n",
        "input_sequences = sequences[:, :-1]\n",
        "target_sequences = sequences[:, 1:]\n",
        "\n",
        "# 클래스의 개수 정의\n",
        "num_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 언어 모델 아키텍처 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "input_sequences = input_sequences.reshape(-1, 50, 1)\n",
        "model.fit(input_sequences, tf.keras.utils.to_categorical(target_sequences, num_classes=num_classes), batch_size=128, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8QT7vDXfnR",
        "outputId": "a9759b3b-2a23-42a0-b355-d11458953b8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 12.5328 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 12.5202 - accuracy: 0.0600\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 12.4993 - accuracy: 0.0400\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.4658 - accuracy: 0.0200\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.4161 - accuracy: 0.0800\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.3549 - accuracy: 0.0600\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.2707 - accuracy: 0.0400\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 12.1792 - accuracy: 0.0600\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.0677 - accuracy: 0.0800\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.9525 - accuracy: 0.0800\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.8213 - accuracy: 0.0600\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 11.6741 - accuracy: 0.0600\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 11.5208 - accuracy: 0.0600\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 11.3696 - accuracy: 0.0400\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 1s 923ms/step - loss: 11.1739 - accuracy: 0.0400\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 10.9935 - accuracy: 0.0600\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 10.7768 - accuracy: 0.0600\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 1s 978ms/step - loss: 10.5925 - accuracy: 0.0600\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 10.3642 - accuracy: 0.0600\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 10.1623 - accuracy: 0.0600\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.9606 - accuracy: 0.0600\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.7528 - accuracy: 0.0600\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.4783 - accuracy: 0.0600\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 9.3026 - accuracy: 0.0600\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 9.0574 - accuracy: 0.0600\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 8.9290 - accuracy: 0.0600\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 8.7079 - accuracy: 0.0600\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 1s 953ms/step - loss: 8.4552 - accuracy: 0.0600\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.2156 - accuracy: 0.0600\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 1s 970ms/step - loss: 8.0150 - accuracy: 0.0600\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.8500 - accuracy: 0.0600\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 7.6147 - accuracy: 0.0600\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.4587 - accuracy: 0.0600\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 7.2068 - accuracy: 0.0600\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 7.0228 - accuracy: 0.0600\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 6.9234 - accuracy: 0.0600\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 6.6369 - accuracy: 0.0600\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 1s 842ms/step - loss: 6.5027 - accuracy: 0.0600\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 6.2067 - accuracy: 0.0600\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 6.1002 - accuracy: 0.0600\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 5.8570 - accuracy: 0.0600\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.6979 - accuracy: 0.0400\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 1s 945ms/step - loss: 5.5355 - accuracy: 0.0400\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 5.3706 - accuracy: 0.0400\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.2030 - accuracy: 0.0200\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.0939 - accuracy: 0.0400\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.9082 - accuracy: 0.0400\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.8068 - accuracy: 0.0800\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.6454 - accuracy: 0.0800\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.5368 - accuracy: 0.0600\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 4.5144 - accuracy: 0.0600\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 4.3630 - accuracy: 0.0600\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 4.2708 - accuracy: 0.0600\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.2892 - accuracy: 0.0600\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.1886 - accuracy: 0.0600\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 4.1198 - accuracy: 0.0600\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 4.0699 - accuracy: 0.0600\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.9931 - accuracy: 0.0600\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.9956 - accuracy: 0.0600\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.9304 - accuracy: 0.0600\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.9072 - accuracy: 0.0600\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8841 - accuracy: 0.0600\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8941 - accuracy: 0.0600\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8369 - accuracy: 0.0600\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 1s 957ms/step - loss: 3.8310 - accuracy: 0.0600\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.8142 - accuracy: 0.0600\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 3.8050 - accuracy: 0.0600\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 1s 917ms/step - loss: 3.7883 - accuracy: 0.0600\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7700 - accuracy: 0.0600\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7781 - accuracy: 0.0600\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7497 - accuracy: 0.0600\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7751 - accuracy: 0.0600\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.7349 - accuracy: 0.0600\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.7440 - accuracy: 0.0600\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 3.7399 - accuracy: 0.0600\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 3.7103 - accuracy: 0.0600\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 1s 923ms/step - loss: 3.7290 - accuracy: 0.0600\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.7122 - accuracy: 0.0600\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 1s 925ms/step - loss: 3.6949 - accuracy: 0.0800\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6798 - accuracy: 0.0600\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 1s 924ms/step - loss: 3.7087 - accuracy: 0.0600\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6917 - accuracy: 0.0600\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 3.7026 - accuracy: 0.0600\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6895 - accuracy: 0.0800\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6932 - accuracy: 0.1000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6818 - accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6896 - accuracy: 0.0800\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6768 - accuracy: 0.0200\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6905 - accuracy: 0.0800\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 1s 959ms/step - loss: 3.6638 - accuracy: 0.0600\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6727 - accuracy: 0.1000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6692 - accuracy: 0.1200\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 1s 910ms/step - loss: 3.6705 - accuracy: 0.0800\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6742 - accuracy: 0.0400\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6699 - accuracy: 0.0200\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 3.6545 - accuracy: 0.0800\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.6633 - accuracy: 0.0800\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6635 - accuracy: 0.0600\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.6756 - accuracy: 0.0600\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 1s 979ms/step - loss: 3.6565 - accuracy: 0.0800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbc3351fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bymsz4vwY9ZP",
        "outputId": "a86db716-2686-400b-9f80-0417c3bd9387"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.sequential.Sequential object at 0x7efbcffc0760>\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 50, 100)           40800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50, 100)           0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 50, 100)           80400     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 50, 100)           80400     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50, 277589)        28036489  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,238,089\n",
            "Trainable params: 28,238,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 로드\n",
        "with open('sentences.txt', 'r') as f:\n",
        "    test_data = f.read()\n",
        "\n",
        "# 토큰화를 수행하기 전에 데이터셋을 제한하기 위해 num_words 매개 변수를 사용합니다.\n",
        "tokenizer = Tokenizer(num_words=277589)\n",
        "tokenizer.fit_on_texts([test_data])\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "test_sequences = tokenizer.texts_to_sequences([test_data])[0]\n",
        "\n",
        "# 테스트 데이터가 seq_length보다 짧을 경우 패딩\n",
        "if len(test_sequences) < seq_length:\n",
        "    test_sequences = np.pad(test_sequences, (seq_length - len(test_sequences), 0), 'constant', constant_values=0)\n",
        "\n",
        "test_sequences = tf.keras.preprocessing.sequence.pad_sequences([test_sequences], maxlen=seq_length, padding='pre')\n",
        "\n",
        "# 입력 데이터와 출력 데이터의 shape을 변경\n",
        "test_input = test_sequences[:, :seq_length, np.newaxis]\n",
        "test_target = tf.keras.utils.to_categorical(test_sequences[:, :seq_length], num_classes=tokenizer.num_words)\n",
        "print(tokenizer.num_words)\n",
        "print(test_input.shape)\n",
        "print(test_target.shape)\n",
        "\n",
        "# 모델 평가\n",
        "model.evaluate(test_input, test_target, verbose=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfYUV3ZZX8k",
        "outputId": "b593f596-9583-4efc-d9cd-d2cbc02aab1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "277589\n",
            "(1, 50, 1)\n",
            "(1, 50, 277589)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13.403054237365723, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수 정의\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        # seed_text를 숫자 시퀀스로 변환\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "        token_list = np.reshape(token_list, (1, max_sequence_len, 1))\n",
        "\n",
        "        # 모델 예측\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "        \n",
        "        # 예측 결과를 숫자에서 단어로 변환\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index in predicted[0]:\n",
        "                output_word = word\n",
        "                break\n",
        "        \n",
        "        # 다음 입력 시퀀스 생성\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# 예측 실행\n",
        "generated_text = generate_text(\"the cat\", 10, model, seq_length)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP8PkXh83Voc",
        "outputId": "c8468a09-1667-4dd0-f263-69425603c0db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat her her her her her her her her her her\n"
          ]
        }
      ]
    }
  ]
}