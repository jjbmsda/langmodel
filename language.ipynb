{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dmsbq-1OiurAEhN_-Yk63kl3oGvLQYSQ",
      "authorship_tag": "ABX9TyNNLO08z4MqyH144/iSdXEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/langmodel/blob/main/language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://www.example.com'\n",
        "response = requests.get(url)\n",
        "text_data = response.text\n",
        "\n",
        "# 텍스트 데이터를 파일로 저장\n",
        "with open('text_data.txt', 'w') as f:\n",
        "    f.write(text_data)\n"
      ],
      "metadata": {
        "id": "EJMmelM7vs8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('sentences.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# 소문자 변환\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# 구두점 및 불필요한 공백 제거\n",
        "text_data = re.sub('[%s]' % re.escape(string.punctuation), '', text_data)\n",
        "text_data = re.sub('\\s+', ' ', text_data)\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "# 입력 시퀀스 생성\n",
        "seq_length = 50\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences], maxlen=seq_length+1, padding='pre')\n",
        "\n",
        "# 입력 시퀀스와 타깃 시퀀스 생성\n",
        "input_sequences = sequences[:, :-1]\n",
        "target_sequences = sequences[:, 1:]\n",
        "\n",
        "# 클래스의 개수 정의\n",
        "num_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 언어 모델 아키텍처 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "input_sequences = input_sequences.reshape(-1, 50, 1)\n",
        "model.fit(input_sequences, tf.keras.utils.to_categorical(target_sequences, num_classes=num_classes), batch_size=128, epochs=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUSKwaaz9Fw",
        "outputId": "ee4993dd-c4ce-4a3b-a67f-01b4b3702cf9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 7.8582 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 7.8163 - accuracy: 0.0800\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 7.7668 - accuracy: 0.1000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 7.7010 - accuracy: 0.1000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 7.6154 - accuracy: 0.1000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 7.5105 - accuracy: 0.1000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 7.3880 - accuracy: 0.1000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 7.2504 - accuracy: 0.1000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 7.0994 - accuracy: 0.1000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 6.9369 - accuracy: 0.1000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 6.7641 - accuracy: 0.1000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 6.5819 - accuracy: 0.1000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 6.3907 - accuracy: 0.1000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 6.1911 - accuracy: 0.1000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 5.9847 - accuracy: 0.1000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 5.7742 - accuracy: 0.1000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 5.5630 - accuracy: 0.1000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 5.3550 - accuracy: 0.1000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 5.1543 - accuracy: 0.1000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 4.9646 - accuracy: 0.1000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 4.7889 - accuracy: 0.1000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 4.6294 - accuracy: 0.1000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4.4874 - accuracy: 0.1000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 4.3630 - accuracy: 0.1000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 4.2555 - accuracy: 0.1000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 4.1635 - accuracy: 0.1000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 4.0851 - accuracy: 0.1000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 4.0183 - accuracy: 0.1000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 3.9614 - accuracy: 0.1000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.9129 - accuracy: 0.1000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.8712 - accuracy: 0.1000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.8353 - accuracy: 0.1000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.8043 - accuracy: 0.1000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.7774 - accuracy: 0.1000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.7541 - accuracy: 0.1000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.7338 - accuracy: 0.1000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.7162 - accuracy: 0.1000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.7009 - accuracy: 0.1000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.6876 - accuracy: 0.1000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 3.6761 - accuracy: 0.1000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.6661 - accuracy: 0.1000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.6574 - accuracy: 0.1000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.6498 - accuracy: 0.1000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.6433 - accuracy: 0.1000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 3.6376 - accuracy: 0.1000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.6327 - accuracy: 0.1000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.6284 - accuracy: 0.1000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.6246 - accuracy: 0.1000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 3.6213 - accuracy: 0.1000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 3.6182 - accuracy: 0.1000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 3.6153 - accuracy: 0.1000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 3.6126 - accuracy: 0.1000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.6099 - accuracy: 0.1000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 3.6073 - accuracy: 0.1000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.6046 - accuracy: 0.1000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 3.6019 - accuracy: 0.1000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 3.5991 - accuracy: 0.1000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 3.5964 - accuracy: 0.1000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.5936 - accuracy: 0.1000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 3.5909 - accuracy: 0.1000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 3.5882 - accuracy: 0.1000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 3.5858 - accuracy: 0.1000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 3.5836 - accuracy: 0.1000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.5819 - accuracy: 0.1000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 3.5794 - accuracy: 0.1000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 3.5770 - accuracy: 0.1000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 3.5747 - accuracy: 0.1000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 3.5725 - accuracy: 0.1000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.5704 - accuracy: 0.1000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.5682 - accuracy: 0.1000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 3.5659 - accuracy: 0.1000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 3.5635 - accuracy: 0.1000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.5609 - accuracy: 0.1000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 3.5579 - accuracy: 0.1000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 3.5700 - accuracy: 0.1000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 3.5676 - accuracy: 0.1000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 3.5647 - accuracy: 0.1000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 3.5617 - accuracy: 0.1000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 3.5584 - accuracy: 0.1000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 3.5551 - accuracy: 0.1000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.5517 - accuracy: 0.1000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 3.5483 - accuracy: 0.1000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 3.5449 - accuracy: 0.1000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.5411 - accuracy: 0.1000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 3.5393 - accuracy: 0.1000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 3.5342 - accuracy: 0.1200\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 3.5310 - accuracy: 0.1200\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 3.5266 - accuracy: 0.1200\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 3.5217 - accuracy: 0.1200\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 3.5207 - accuracy: 0.1200\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 3.5132 - accuracy: 0.1200\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 3.5104 - accuracy: 0.1400\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 3.5066 - accuracy: 0.1400\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 3.5023 - accuracy: 0.1400\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 3.4975 - accuracy: 0.1400\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 3.4912 - accuracy: 0.1400\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 3.4833 - accuracy: 0.1400\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 3.4772 - accuracy: 0.1400\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 3.4712 - accuracy: 0.1400\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 3.4651 - accuracy: 0.1200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbcceba070>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('sentences.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# 소문자 변환\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# 구두점 및 불필요한 공백 제거\n",
        "text_data = re.sub('[%s]' % re.escape(string.punctuation), '', text_data)\n",
        "text_data = re.sub('\\s+', ' ', text_data)\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "# 입력 시퀀스 생성\n",
        "seq_length = 50\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences], maxlen=seq_length+1, padding='pre')\n",
        "\n",
        "# 입력 시퀀스와 타깃 시퀀스 생성\n",
        "input_sequences = sequences[:, :-1]\n",
        "target_sequences = sequences[:, 1:]\n",
        "\n",
        "# 클래스의 개수 정의\n",
        "num_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 언어 모델 아키텍처 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "input_sequences = input_sequences.reshape(-1, 50, 1)\n",
        "model.fit(input_sequences, tf.keras.utils.to_categorical(target_sequences, num_classes=num_classes), batch_size=128, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8QT7vDXfnR",
        "outputId": "434e9029-107d-4a0b-9224-4424fe299fb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 7.8736 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 7.8416 - accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 7.8062 - accuracy: 0.0800\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 7.7613 - accuracy: 0.1000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 7.6802 - accuracy: 0.0800\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 7.5943 - accuracy: 0.1200\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 7.4902 - accuracy: 0.0600\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 7.3616 - accuracy: 0.0600\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 7.2537 - accuracy: 0.0800\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 7.0994 - accuracy: 0.1000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 6.9459 - accuracy: 0.0400\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 6.7993 - accuracy: 0.0600\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 6.6166 - accuracy: 0.1200\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 6.4242 - accuracy: 0.1000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 6.2330 - accuracy: 0.0800\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 6.0476 - accuracy: 0.0800\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 5.8217 - accuracy: 0.1000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 5.6289 - accuracy: 0.0800\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 5.4001 - accuracy: 0.0600\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 5.2332 - accuracy: 0.0600\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 5.0594 - accuracy: 0.0600\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 4.8660 - accuracy: 0.0600\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 4.7532 - accuracy: 0.1000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 4.5404 - accuracy: 0.1000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 4.4162 - accuracy: 0.0800\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 4.2942 - accuracy: 0.0200\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 4.1618 - accuracy: 0.0800\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 4.1033 - accuracy: 0.0400\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 3.9759 - accuracy: 0.0800\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 3.9890 - accuracy: 0.0400\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 3.9335 - accuracy: 0.0600\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 3.9265 - accuracy: 0.0600\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 3.8622 - accuracy: 0.0800\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 3.8051 - accuracy: 0.0600\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 3.8121 - accuracy: 0.0600\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 3.7898 - accuracy: 0.0800\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 3.7809 - accuracy: 0.0600\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 3.7132 - accuracy: 0.1000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 3.6942 - accuracy: 0.1400\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.7256 - accuracy: 0.1000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 3.6699 - accuracy: 0.1000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 3.6528 - accuracy: 0.1000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.6747 - accuracy: 0.1000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 3.6506 - accuracy: 0.1000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.6549 - accuracy: 0.1000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 3.6512 - accuracy: 0.1000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 3.6375 - accuracy: 0.1200\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 3.6349 - accuracy: 0.0800\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 3.6149 - accuracy: 0.0600\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 3.6534 - accuracy: 0.0600\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 3.6030 - accuracy: 0.0800\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 3.6735 - accuracy: 0.1000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 3.6044 - accuracy: 0.1200\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 3.6463 - accuracy: 0.1000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 3.6440 - accuracy: 0.0800\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 3.6281 - accuracy: 0.1000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 3.6347 - accuracy: 0.0800\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.5880 - accuracy: 0.1000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 3.6471 - accuracy: 0.1000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.5769 - accuracy: 0.1000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 3.5781 - accuracy: 0.1000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.5953 - accuracy: 0.1000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 3.6012 - accuracy: 0.1200\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 3.6401 - accuracy: 0.1000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.6052 - accuracy: 0.1000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 3.5874 - accuracy: 0.1000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 3.5767 - accuracy: 0.1000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 3.5966 - accuracy: 0.1200\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.5849 - accuracy: 0.1000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.6113 - accuracy: 0.0800\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.6055 - accuracy: 0.0800\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.5761 - accuracy: 0.1000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.6079 - accuracy: 0.1000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.5939 - accuracy: 0.1000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.6343 - accuracy: 0.1000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 3.5852 - accuracy: 0.1200\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 3.5941 - accuracy: 0.0800\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 3.5897 - accuracy: 0.1000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 3.5767 - accuracy: 0.1200\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 3.5798 - accuracy: 0.1000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 3.5855 - accuracy: 0.1000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.5668 - accuracy: 0.1400\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 3.5625 - accuracy: 0.0800\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 3.5506 - accuracy: 0.0800\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 3.5799 - accuracy: 0.1000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 3.5244 - accuracy: 0.1000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.5560 - accuracy: 0.1400\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.5611 - accuracy: 0.1000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.5534 - accuracy: 0.1000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.5639 - accuracy: 0.1000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.5970 - accuracy: 0.0800\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 3.5569 - accuracy: 0.1000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.5673 - accuracy: 0.0800\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.5226 - accuracy: 0.1200\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.5504 - accuracy: 0.1000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 3.5410 - accuracy: 0.1000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.5498 - accuracy: 0.1000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 3.6302 - accuracy: 0.1000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 3.5341 - accuracy: 0.1000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 3.5291 - accuracy: 0.1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc33a15ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bymsz4vwY9ZP",
        "outputId": "2c270118-da15-44b6-c617-10f621616334"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.sequential.Sequential object at 0x7efbbb1bc460>\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 50, 100)           40800     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 50, 100)           80400     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              (None, 50, 100)           80400     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 50, 2618)          264418    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 466,018\n",
            "Trainable params: 466,018\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "with open('wikisent2.txt', 'r') as f:\n",
        "    test_data = f.read()\n",
        "\n",
        "# 소문자 변환\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# 구두점 및 불필요한 공백 제거\n",
        "text_data = re.sub('[%s]' % re.escape(string.punctuation), '', text_data)\n",
        "text_data = re.sub('\\s+', ' ', text_data)\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "test_sequences = tokenizer.texts_to_sequences([test_data])[0]\n",
        "\n",
        "# 테스트 데이터가 seq_length보다 짧을 경우 패딩\n",
        "if len(test_sequences) < seq_length:\n",
        "    test_sequences = np.pad(test_sequences, (seq_length - len(test_sequences), 0), 'constant', constant_values=0)\n",
        "\n",
        "test_sequences = tf.keras.preprocessing.sequence.pad_sequences([test_sequences], maxlen=seq_length, padding='pre')\n",
        "\n",
        "# 입력 데이터와 출력 데이터의 shape을 변경\n",
        "test_input = test_sequences[:, :seq_length, np.newaxis]\n",
        "test_target = tf.keras.utils.to_categorical(test_sequences[:, :seq_length], num_classes=2618)\n",
        "print(tokenizer.num_words)\n",
        "print(test_input.shape)\n",
        "print(test_target.shape)\n",
        "\n",
        "# 모델 평가\n",
        "model.evaluate(test_input, test_target, verbose=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfYUV3ZZX8k",
        "outputId": "ed866a78-0a95-4a22-a8fe-e59b86666c64"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "(1, 50, 1)\n",
            "(1, 50, 2618)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9.640216827392578, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수 정의\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        # seed_text를 숫자 시퀀스로 변환\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "        token_list = np.reshape(token_list, (1, max_sequence_len, 1))\n",
        "\n",
        "        # 모델 예측\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "        \n",
        "        # 예측 결과를 숫자에서 단어로 변환\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index in predicted[0]:\n",
        "                output_word = word\n",
        "                break\n",
        "        \n",
        "        # 다음 입력 시퀀스 생성\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# 예측 실행\n",
        "generated_text = generate_text(\"the cat\", 10, model, seq_length)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP8PkXh83Voc",
        "outputId": "68448e5b-8831-440b-ab5b-b660eee66a05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat my my my my my my my my my my\n"
          ]
        }
      ]
    }
  ]
}